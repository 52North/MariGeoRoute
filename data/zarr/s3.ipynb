{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maridata S3 ODC ZARR Experiments\n",
    "\n",
    "* Contact: SSH, EHJ\n",
    "* Requirements: Python, venv\n",
    "* Credentials required for accessing the S3 bucket are available via the project password safe\n",
    "  * Credentials should be provided using a `.env` file with the following entries: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_BUCKET_REGION`.\n",
    "    The first two are supported names of boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zarr boto3 dotenv xarray ipykernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Contact With a S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "session = boto3.Session(aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                        region_name=os.getenv('AWS_BUCKET_REGION'))\n",
    "s3 = session.client('s3')\n",
    "\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_buckets.html\n",
    "#\n",
    "buckets = s3.list_buckets()\n",
    "print(f\"The first bucket: '{buckets['Buckets'][0]}\")\n",
    "bucket = buckets['Buckets'][0]['Name']\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "#\n",
    "obj_key = 'uploaded/s3/new/path/requirements.txt'\n",
    "s3.upload_file(Filename='requirements.txt', Bucket=bucket, Key=obj_key)\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_objects_v2.html\n",
    "#\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix='zarr', MaxKeys=10)\n",
    "objs\n",
    "objs2 = s3.list_objects_v2(Bucket=bucket, MaxKeys=16, ContinuationToken=objs['NextContinuationToken'])\n",
    "objs2\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_object.html\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_objects.html\n",
    "#\n",
    "delete_response = s3.delete_object(Bucket=bucket, Key='uploaded/s3/new/path/requirements.txt')\n",
    "delete_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NETCDF to ZARR Using xarray\n",
    "\n",
    "* Specification: <https://zarr.readthedocs.io/en/stable/spec/v2.html>\n",
    "* Python libraries:\n",
    "  * zarr: [api docs](https://zarr.readthedocs.io/en/stable/api.html), [@pypi](https://pypi.org/project/zarr/)\n",
    "  * xarray: [api docs](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html#xarray.Dataset.to_zarr), [@pypi](https://pypi.org/project/xarray/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/getting-started-guide/faq.html#id6\n",
    "# https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html#xarray.open_dataset\n",
    "#\n",
    "ds = xr.open_dataset(\"./data/simu_alternateWaves_20230421_20230425.nc\", engine=\"netcdf4\")\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/user-guide/io.html#io-zarr\n",
    "# https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html#xarray.Dataset.to_zarr\n",
    "#\n",
    "zarr_store = './zarr/'\n",
    "ds.to_zarr(store=zarr_store, mode='w', consolidated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Zarr Store to S3 Bucket via Boto3\n",
    "\n",
    "* Documentation: <https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object.html>\n",
    "* AWS storage classes for s3 objects: <https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ds = xr.open_dataset(\"./data/simu_alternateWaves_20230421_20230425.nc\", engine=\"netcdf4\")\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                        region_name=os.getenv('AWS_BUCKET_REGION'))\n",
    "s3 = session.client('s3')\n",
    "\n",
    "zarr_store = './zarr/'\n",
    "bucket = '52n-maridata'\n",
    "store = zarr.DirectoryStore(zarr_store)\n",
    "\n",
    "ds.to_zarr(store=zarr_store, mode='w', consolidated=True)\n",
    "\n",
    "zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "#s3_keys = []\n",
    "for filename in store.keys():\n",
    "    s3_key = zarr_prefix + filename\n",
    "    #s3_keys.append(s3_key)\n",
    "    local_filename = 'zarr/'+filename\n",
    "    print(f\"Upload local file '{local_filename}' to s3 key 's3://{bucket}/{s3_key}\")\n",
    "    s3.upload_file(Filename=local_filename, Bucket=bucket, Key=s3_key)\n",
    "\n",
    "#s3.delete_objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access S3 Bucket via s3fs\n",
    "\n",
    "* Documenation: <https://s3fs.readthedocs.io/en/latest/>\n",
    "* Python library: [api docs](https://s3fs.readthedocs.io/en/latest/api.html), [@pypi](https://pypi.org/project/s3fs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "\n",
    "#load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#load netCDF4 file using xarray \n",
    "ds = xr.open_dataset(\"./data/simu_alternateWaves_20230421_20230425.nc\", engine=\"netcdf4\")\n",
    "\n",
    "#   https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem\n",
    "#   https://github.com/fsspec/s3fs/blob/main/s3fs/core.py\n",
    "\n",
    "#Setup the s3 file system by providing necessary credentials\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv('AWS_ACCESS_KEY_ID'), \n",
    "                       secret=os.getenv('AWS_SECRET_ACCESS_KEY'),client_kwargs={'region_name':os.getenv('AWS_BUCKET_REGION')})\n",
    "\n",
    "#s3.ls('52n-maridata') #check the bucket in s3 file system\n",
    "#provide the bucket name and the zarr prefix for s3\n",
    "bucket = '52n-maridata'\n",
    "zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "\n",
    "#create store using S3Map method from s3fs library and save the dataset as a zarr directory/store in s3 bucket\n",
    "store = s3fs.S3Map(root=f\"s3://{bucket}/{zarr_prefix}\", s3=s3)\n",
    "ds.to_zarr(store=store, mode='w', consolidated=True) \n",
    "\n",
    "#   List S3 bucket content\n",
    "#\n",
    "# zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "# s3_fs.ls(f\"{bucket}/{zarr_prefix}\")\n",
    "#\n",
    "# folder is created but not all the other files\n",
    "#with s3_fs.open(f\"{bucket}/{zarr_prefix}\", 'wb') as s3_folder:\n",
    "#    store = zarr.DirectoryStore(s3_folder.path)\n",
    "#    ds.to_zarr(store, mode='w', consolidated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenDataCube and ZARR\n",
    "\n",
    "* build `datacube-zarr` from source: <https://github.com/opendatacube/datacube-zarr>\n",
    "* fix version of gdal to match the version installed on the system: `gdalinfo --version`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install gdal-bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opendatacube/datacube-zarr\n",
    "%pip install --upgrade gdal==3.4.1\n",
    "%pip install --upgrade --editable \"./.[test]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataset metadata to the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import yaml\n",
    "import boto3\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "df = xr.open_dataset('./data/waves1.nc')\n",
    "df\n",
    "\n",
    "# Generate a random UUID\n",
    "id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def generate_measurement_dict():\n",
    "    t = {}\n",
    "    for i in range(len(list(df.data_vars)[:-1])):\n",
    "        a = {str(list(df.data_vars)[:-1][i]): {\n",
    "            'path': 's3://52n-maridata/opendatacube/zarr/forecast/waves/',\n",
    "            'layer': str(list(df.data_vars)[:-1][i]),\n",
    "        }}\n",
    "        t = a | t\n",
    "    return t\n",
    "\n",
    "\n",
    "# Define the metadata information\n",
    "metadata = {\n",
    "    'id': '272302c9-1449-4a33-8166-4b6083a8a715',\n",
    "    '$schema': 'https://schemas.opendatacube.org/dataset',\n",
    "    'product': {\n",
    "        'name': 'waves',\n",
    "    },\n",
    "    'crs': 'epsg:4326',\n",
    "    'geometry':\n",
    "        {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[[-90.0, -180.0], [-90.0, 180.0], [90.0, 180.0], [90.0, -180.0], [-90.0, -180.0]]]\n",
    "    },\n",
    "    'grids': {\n",
    "        'default': {\n",
    "            'shape': [df['VHM0'].shape[2], df['VHM0'].shape[1]],\n",
    "            'transform': list(df['VHM0'].rio.transform()),\n",
    "        },\n",
    "    },\n",
    "    'measurements': generate_measurement_dict(),\n",
    "    'properties': {\n",
    "        'eo:platform': 'na',\n",
    "        'eo:instrument': 'na',\n",
    "        'datetime': str(df.time.values[0]),\n",
    "        'odc:processing_datetime': str(df.time.values[0]),\n",
    "        'odc:file_format': 'Zarr',\n",
    "    },\n",
    "    'lineage': {},\n",
    "}\n",
    "\n",
    "# Print the metadata information\n",
    "# print(metadata)\n",
    "# Define S3 bucket and key\n",
    "bucket_name = '52n-maridata'\n",
    "key = f'opendatacube/zarr/forecast/waves/eo3_{metadata[\"product\"][\"name\"]}_dataset.yaml'\n",
    "\n",
    "# Convert the dictionary to a YAML string\n",
    "yaml_str = yaml.dump(metadata, default_flow_style=False,\n",
    "                     line_break='\\n', allow_unicode=True)\n",
    "\n",
    "# Upload YAML file to S3 bucket\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'), \n",
    "    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.environ.get('AWS_DEFAULT_REGION'))\n",
    "\n",
    "s3 = session.client('s3')\n",
    "# Print the YAML \n",
    "s3.put_object(Body=yaml_str, Bucket=bucket_name, Key=key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index metadata from s3 to the opendatacube\n",
    "```\n",
    "datacube -v system init\n",
    "datacube product add s3://52n-maridata/opendatacube/zarr/forecast/waves/eo3_waves_product.yaml\n",
    "datacube dataset add s3://52n-maridata/opendatacube/za```rr/forecast/waves/eo3_waves_dataset.yaml\n",
    "``` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and initialize opendatacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube \n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = dc.list_products()\n",
    "product_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_list = dc.list_measurements()\n",
    "measurement_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available datasets/measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=measurement_list.to_dict()\n",
    "vars= list(a['name'].values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_datacube = dc.load('waves',measurements=list(a['name'].values()),output_crs = \"epsg:4326\",resolution = (1, 1),align = (0.5, 0.5),crs='EPSG:4326')\n",
    "ds_datacube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "for var in vars:\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ds_datacube[var].plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
