{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maridata S3 ODC ZARR Experiments\n",
    "\n",
    "* Contact: SSH, EHJ\n",
    "* Requirements: Python, venv\n",
    "* Credentials required for accessing the S3 bucket are available via the project password safe\n",
    "  * Credentials should be provided using a `.env` file with the following entries: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_BUCKET_REGION`.\n",
    "    The first two are supported names of boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv --prompt odc-s3-zarr .venv\n",
    "!source .venv/bin/activate\n",
    "%pip install zarr boto3 python-dotenv xarray ipykernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Contact With a S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "session = boto3.Session(aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                        region_name=os.getenv('AWS_BUCKET_REGION'))\n",
    "s3 = session.client('s3')\n",
    "\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_buckets.html\n",
    "#\n",
    "buckets = s3.list_buckets()\n",
    "print(f\"The first bucket: '{buckets['Buckets'][0]}\")\n",
    "bucket = buckets['Buckets'][0]['Name']\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "#\n",
    "obj_key = 'uploaded/s3/new/path/requirements.txt'\n",
    "s3.upload_file(Filename='requirements.txt', Bucket=bucket, Key=obj_key)\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_objects_v2.html\n",
    "#\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix='zarr', MaxKeys=10)\n",
    "objs\n",
    "objs2 = s3.list_objects_v2(Bucket=bucket, MaxKeys=16, ContinuationToken=objs['NextContinuationToken'])\n",
    "objs2\n",
    "#\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_object.html\n",
    "#   https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_objects.html\n",
    "#\n",
    "delete_response = s3.delete_object(Bucket=bucket, Key='uploaded/s3/new/path/requirements.txt')\n",
    "delete_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NETCDF to ZARR Using xarray\n",
    "\n",
    "* Specification: <https://zarr.readthedocs.io/en/stable/spec/v2.html>\n",
    "* Python libraries:\n",
    "  * zarr: [api docs](https://zarr.readthedocs.io/en/stable/api.html), [@pypi](https://pypi.org/project/zarr/)\n",
    "  * xarray: [api docs](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html#xarray.Dataset.to_zarr), [@pypi](https://pypi.org/project/xarray/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/getting-started-guide/faq.html#id6\n",
    "# https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html#xarray.open_dataset\n",
    "#\n",
    "ds = xr.open_dataset(\"./data/waves1.nc\", engine=\"netcdf4\", decode_coords='all')\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/user-guide/io.html#io-zarr\n",
    "# https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html#xarray.Dataset.to_zarr\n",
    "#\n",
    "zarr_store = './zarr/'\n",
    "ds.to_zarr(store=zarr_store, mode='w', consolidated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Zarr Store to S3 Bucket via Boto3\n",
    "\n",
    "* Documentation: <https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object.html>\n",
    "* AWS storage classes for s3 objects: <https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ds = xr.open_dataset(\"./data/waves1.nc\", engine=\"netcdf4\", decode_coords='all')\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                        region_name=os.getenv('AWS_BUCKET_REGION'))\n",
    "s3 = session.client('s3')\n",
    "\n",
    "zarr_store = './zarr/'\n",
    "bucket = '52n-maridata'\n",
    "store = zarr.DirectoryStore(zarr_store)\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/user-guide/io.html#consolidated-metadata\n",
    "#\n",
    "ds.to_zarr(store=zarr_store, mode='w', consolidated=True)\n",
    "\n",
    "zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "#s3_keys = []\n",
    "for filename in store.keys():\n",
    "    s3_key = zarr_prefix + filename\n",
    "    #s3_keys.append(s3_key)\n",
    "    local_filename = 'zarr/'+filename\n",
    "    print(f\"Upload local file '{local_filename}' to s3 key 's3://{bucket}/{s3_key}\")\n",
    "    s3.upload_file(Filename=local_filename, Bucket=bucket, Key=s3_key)\n",
    "\n",
    "#s3.delete_objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access S3 Bucket via s3fs\n",
    "\n",
    "* Documenation: <https://s3fs.readthedocs.io/en/latest/>\n",
    "* Python library: [api docs](https://s3fs.readthedocs.io/en/latest/api.html), [@pypi](https://pypi.org/project/s3fs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "\n",
    "#load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#load netCDF4 file using xarray \n",
    "ds = xr.open_dataset(\"./data/waves1.nc\", engine=\"netcdf4\", decode_cords='all')\n",
    "\n",
    "#   https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem\n",
    "#   https://github.com/fsspec/s3fs/blob/main/s3fs/core.py\n",
    "\n",
    "#Setup the s3 file system by providing necessary credentials\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv('AWS_ACCESS_KEY_ID'), \n",
    "                       secret=os.getenv('AWS_SECRET_ACCESS_KEY'),client_kwargs={'region_name':os.getenv('AWS_BUCKET_REGION')})\n",
    "\n",
    "#s3.ls('52n-maridata') #check the bucket in s3 file system\n",
    "#provide the bucket name and the zarr prefix for s3\n",
    "bucket = '52n-maridata'\n",
    "zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "\n",
    "#create store using S3Map method from s3fs library and save the dataset as a zarr directory/store in s3 bucket\n",
    "store = s3fs.S3Map(root=f\"s3://{bucket}/{zarr_prefix}\", s3=s3)\n",
    "#\n",
    "# https://docs.xarray.dev/en/stable/user-guide/io.html#consolidated-metadata\n",
    "#\n",
    "ds.to_zarr(store=store, mode='w', consolidated=True) \n",
    "\n",
    "#   List S3 bucket content\n",
    "#\n",
    "# zarr_prefix = 'opendatacube/zarr/forecast/waves/'\n",
    "# s3_fs.ls(f\"{bucket}/{zarr_prefix}\")\n",
    "#\n",
    "# folder is created but not all the other files\n",
    "#with s3_fs.open(f\"{bucket}/{zarr_prefix}\", 'wb') as s3_folder:\n",
    "#    store = zarr.DirectoryStore(s3_folder.path)\n",
    "#    ds.to_zarr(store, mode='w', consolidated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenDataCube and ZARR\n",
    "\n",
    "* build `datacube-zarr` from source: <https://github.com/opendatacube/datacube-zarr>\n",
    "* fix version of gdal to match the version installed on the system: `gdalinfo --version`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install gdal-bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opendatacube/datacube-zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install GDAL==$(gdalinfo --version | awk '{print $2}' | awk -F, '{print $1}') odc-apps-cloud odc-apps-dc-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --upgrade --editable \"./datacube-zarr/.[test]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataset metadata to the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import yaml\n",
    "import boto3\n",
    "import rioxarray\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "# Loading the global forecast netCDF dataset downloaded from Copernicus marine datasets\n",
    "# https://data.marine.copernicus.eu/products\n",
    "df = xr.open_dataset('./data/waves1.nc', decode_coords='all')\n",
    "df\n",
    "\n",
    "# Generate a random UUID\n",
    "id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def generate_measurement_dict():\n",
    "    t = {}\n",
    "    for i in range(len(list(df.data_vars)[:-1])):\n",
    "        a = {str(list(df.data_vars)[:-1][i]): {\n",
    "            'path': 's3://52n-maridata/opendatacube/zarr/forecast/waves/',\n",
    "            'layer': str(list(df.data_vars)[:-1][i]),\n",
    "        }}\n",
    "        t = a | t\n",
    "    return t\n",
    "\n",
    "\n",
    "# Define the metadata information\n",
    "metadata = {\n",
    "    'id': '272302c9-1449-4a33-8166-4b6083a8a715',\n",
    "    '$schema': 'https://schemas.opendatacube.org/dataset',\n",
    "    'product': {\n",
    "        'name': 'waves',\n",
    "    },\n",
    "    'crs': 'epsg:4326',\n",
    "    'geometry':\n",
    "        {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[[-90.0, -180.0], [-90.0, 180.0], [90.0, 180.0], [90.0, -180.0], [-90.0, -180.0]]]\n",
    "    },\n",
    "    'grids': {\n",
    "        'default': {\n",
    "            'shape': [df['VHM0'].shape[2], df['VHM0'].shape[1]],\n",
    "            'transform': list(df['VHM0'].rio.transform()),\n",
    "        },\n",
    "    },\n",
    "    'measurements': generate_measurement_dict(),\n",
    "    'properties': {\n",
    "        'eo:platform': 'na',\n",
    "        'eo:instrument': 'na',\n",
    "        'datetime': str(df.time.values[0]),\n",
    "        'odc:processing_datetime': str(df.time.values[0]),\n",
    "        'odc:file_format': 'Zarr',\n",
    "    },\n",
    "    'lineage': {},\n",
    "}\n",
    "\n",
    "# Print the metadata information\n",
    "# print(metadata)\n",
    "# Define S3 bucket and key\n",
    "bucket_name = '52n-maridata'\n",
    "key = f'opendatacube/zarr/forecast/waves/eo3_{metadata[\"product\"][\"name\"]}_dataset.yaml'\n",
    "\n",
    "# Convert the dictionary to a YAML string\n",
    "yaml_str = yaml.dump(metadata, default_flow_style=False,\n",
    "                     line_break='\\n', allow_unicode=True)\n",
    "\n",
    "# Upload YAML file to S3 bucket\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'), \n",
    "    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.environ.get('AWS_DEFAULT_REGION'))\n",
    "\n",
    "s3 = session.client('s3')\n",
    "# Print the YAML \n",
    "s3.put_object(Body=yaml_str, Bucket=bucket_name, Key=key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Product metadata to the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "df = xr.open_dataset('./data/waves1.nc')\n",
    "df\n",
    "\n",
    "# Define metadata\n",
    "metadata = {\n",
    "    'product': {\n",
    "        'name': 'waves'\n",
    "    }\n",
    "}\n",
    "\n",
    "def var_measurements():\n",
    "    measure = []\n",
    "    for i in range(len(list(df.data_vars)[:-1])):\n",
    "        a = {\n",
    "            'name': str(list(df.data_vars)[:-1][i]),\n",
    "            'dtype': str(df.data_vars[list(df.data_vars)[:-1][i]].dtype),\n",
    "            'nodata': -999,\n",
    "            'units': str(df.data_vars[list(df.data_vars)[:-1][i]].attrs['units']),\n",
    "            'aliases': [str(df.data_vars[list(df.data_vars)[:-1][i]].attrs['standard_name'])]\n",
    "            }\n",
    "        measure.append(a)\n",
    "    return measure\n",
    "\n",
    "# Define YAML data structure\n",
    "data = {\n",
    "    'name': 'waves',\n",
    "    'metadata': metadata,\n",
    "    'description': df.attrs['dataset'],\n",
    "    'metadata_type': 'eo3',\n",
    "    'measurements': var_measurements(),\n",
    "    'storage':\n",
    "        {'crs': 'EPSG:4326',\n",
    "        'resolution': {'latitude' : 1.0,'longitude' : 1.0}\n",
    "}}\n",
    "\n",
    "###### saving locally ###################\n",
    "# class CustomDumper(yaml.Dumper):\n",
    "#     def increase_indent(self, flow=False, indentless=False):\n",
    "#         return super(CustomDumper, self).increase_indent(flow, False)\n",
    "\n",
    "# with open(f'eo3_{data[\"name\"]}_product.yaml', 'w') as file:\n",
    "#     yaml.dump(data, file, default_flow_style=False, sort_keys=False, allow_unicode=True, Dumper=CustomDumper)\n",
    "\n",
    "\n",
    "######## saving to S3 ###############\n",
    "\n",
    "# Convert data to YAML string\n",
    "yaml_data = yaml.dump(data, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "\n",
    "# Define S3 bucket and key\n",
    "bucket_name = '52n-maridata'\n",
    "key = f'opendatacube/zarr/forecast/waves/eo3_{data[\"name\"]}_product.yaml'\n",
    "\n",
    "# Upload YAML file to S3 bucket\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'), \n",
    "    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.environ.get('AWS_DEFAULT_REGION'))\n",
    "\n",
    "s3 = session.client('s3')\n",
    "s3.put_object(Body=yaml_data, Bucket=bucket_name, Key=key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index metadata from s3 to the opendatacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way of indexing datasets (requires some additional python packages to be installed)\n",
    "s3-find \"s3://52n-maridata/opendatacube/*/*/*/*dataset.yaml\" | s3-to-tar | dc-index-from-tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!datacube -v system init\n",
    "!datacube product add s3://52n-maridata/opendatacube/zarr/forecast/waves/eo3_waves_product.yaml\n",
    "!datacube dataset add s3://52n-maridata/opendatacube/zarr/forecast/waves/eo3_waves_dataset.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and initialize opendatacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube \n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = dc.list_products()\n",
    "product_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_list = dc.list_measurements()\n",
    "measurement_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available datasets/measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=measurement_list.to_dict()\n",
    "vars= list(a['name'].values())\n",
    "vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not working because of [Dataset.to_zarr() does not preserve CRS information](https://github.com/pydata/xarray/issues/6288).\n",
    "\n",
    "We were not able to load the NETCDF and convert it to zarr in a way, that it provides the CRS information that is supported by OpenDataCube.\n",
    "\n",
    "Hence, we will be using netcdf on ebs and not s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataArray' object has no attribute 'crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_datacube \u001b[39m=\u001b[39m dc\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mwaves\u001b[39;49m\u001b[39m'\u001b[39;49m, measurements\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(a[\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()), output_crs\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepsg:4326\u001b[39;49m\u001b[39m\"\u001b[39;49m, resolution\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), align\u001b[39m=\u001b[39;49m(\u001b[39m0.5\u001b[39;49m, \u001b[39m0.5\u001b[39;49m),crs\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEPSG:4326\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m ds_datacube\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/api/core.py:443\u001b[0m, in \u001b[0;36mDatacube.load\u001b[0;34m(self, product, measurements, output_crs, resolution, resampling, skip_broken_datasets, dask_chunks, like, fuse_func, align, datasets, dataset_predicate, progress_cbk, patch_url, **query)\u001b[0m\n\u001b[1;32m    439\u001b[0m measurement_dicts \u001b[39m=\u001b[39m datacube_product\u001b[39m.\u001b[39mlookup_measurements(measurements)\n\u001b[1;32m    441\u001b[0m \u001b[39m# `extra_dims` put last for backwards compability, but should really be the second position\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[39m# betwween `grouped` and `geobox`\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data(grouped, geobox,\n\u001b[1;32m    444\u001b[0m                         measurement_dicts,\n\u001b[1;32m    445\u001b[0m                         resampling\u001b[39m=\u001b[39;49mresampling,\n\u001b[1;32m    446\u001b[0m                         fuse_func\u001b[39m=\u001b[39;49mfuse_func,\n\u001b[1;32m    447\u001b[0m                         dask_chunks\u001b[39m=\u001b[39;49mdask_chunks,\n\u001b[1;32m    448\u001b[0m                         skip_broken_datasets\u001b[39m=\u001b[39;49mskip_broken_datasets,\n\u001b[1;32m    449\u001b[0m                         progress_cbk\u001b[39m=\u001b[39;49mprogress_cbk,\n\u001b[1;32m    450\u001b[0m                         extra_dims\u001b[39m=\u001b[39;49mextra_dims,\n\u001b[1;32m    451\u001b[0m                         patch_url\u001b[39m=\u001b[39;49mpatch_url)\n\u001b[1;32m    453\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/api/core.py:801\u001b[0m, in \u001b[0;36mDatacube.load_data\u001b[0;34m(sources, geobox, measurements, resampling, fuse_func, dask_chunks, skip_broken_datasets, progress_cbk, extra_dims, patch_url, **extra)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m Datacube\u001b[39m.\u001b[39m_dask_load(sources, geobox, measurements, dask_chunks,\n\u001b[1;32m    797\u001b[0m                                skip_broken_datasets\u001b[39m=\u001b[39mskip_broken_datasets,\n\u001b[1;32m    798\u001b[0m                                extra_dims\u001b[39m=\u001b[39mextra_dims,\n\u001b[1;32m    799\u001b[0m                                patch_url\u001b[39m=\u001b[39mpatch_url)\n\u001b[1;32m    800\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 801\u001b[0m     \u001b[39mreturn\u001b[39;00m Datacube\u001b[39m.\u001b[39;49m_xr_load(sources, geobox, measurements,\n\u001b[1;32m    802\u001b[0m                              skip_broken_datasets\u001b[39m=\u001b[39;49mskip_broken_datasets,\n\u001b[1;32m    803\u001b[0m                              progress_cbk\u001b[39m=\u001b[39;49mprogress_cbk,\n\u001b[1;32m    804\u001b[0m                              extra_dims\u001b[39m=\u001b[39;49mextra_dims,\n\u001b[1;32m    805\u001b[0m                              patch_url\u001b[39m=\u001b[39;49mpatch_url)\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/api/core.py:729\u001b[0m, in \u001b[0;36mDatacube._xr_load\u001b[0;34m(sources, geobox, measurements, skip_broken_datasets, progress_cbk, extra_dims, patch_url)\u001b[0m\n\u001b[1;32m    727\u001b[0m data_slice \u001b[39m=\u001b[39m data[m\u001b[39m.\u001b[39mname]\u001b[39m.\u001b[39mvalues[index]\n\u001b[1;32m    728\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m     _fuse_measurement(data_slice, datasets, geobox, m,\n\u001b[1;32m    730\u001b[0m                       skip_broken_datasets\u001b[39m=\u001b[39;49mskip_broken_datasets,\n\u001b[1;32m    731\u001b[0m                       progress_cbk\u001b[39m=\u001b[39;49m_cbk, extra_dim_index\u001b[39m=\u001b[39;49mextra_dim_index,\n\u001b[1;32m    732\u001b[0m                       patch_url\u001b[39m=\u001b[39;49mpatch_url)\n\u001b[1;32m    733\u001b[0m \u001b[39mexcept\u001b[39;00m (TerminateCurrentLoad, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    734\u001b[0m     data\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39mdc_partial_load\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/api/core.py:952\u001b[0m, in \u001b[0;36m_fuse_measurement\u001b[0;34m(dest, datasets, geobox, measurement, skip_broken_datasets, progress_cbk, extra_dim_index, patch_url)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    950\u001b[0m         srcs\u001b[39m.\u001b[39mappend(src)\n\u001b[0;32m--> 952\u001b[0m reproject_and_fuse(srcs,\n\u001b[1;32m    953\u001b[0m                    dest,\n\u001b[1;32m    954\u001b[0m                    geobox,\n\u001b[1;32m    955\u001b[0m                    dest\u001b[39m.\u001b[39;49mdtype\u001b[39m.\u001b[39;49mtype(measurement\u001b[39m.\u001b[39;49mnodata),\n\u001b[1;32m    956\u001b[0m                    resampling\u001b[39m=\u001b[39;49mmeasurement\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mresampling_method\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnearest\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    957\u001b[0m                    fuse_func\u001b[39m=\u001b[39;49mmeasurement\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mfuser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    958\u001b[0m                    skip_broken_datasets\u001b[39m=\u001b[39;49mskip_broken_datasets,\n\u001b[1;32m    959\u001b[0m                    progress_cbk\u001b[39m=\u001b[39;49mprogress_cbk,\n\u001b[1;32m    960\u001b[0m                    extra_dim_index\u001b[39m=\u001b[39;49mextra_dim_index)\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/storage/_load.py:78\u001b[0m, in \u001b[0;36mreproject_and_fuse\u001b[0;34m(datasources, destination, dst_gbox, dst_nodata, resampling, fuse_func, skip_broken_datasets, progress_cbk, extra_dim_index)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m ignore_exceptions_if(skip_broken_datasets):\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m datasources[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mopen() \u001b[39mas\u001b[39;00m rdr:\n\u001b[0;32m---> 78\u001b[0m         read_time_slice(rdr, destination, dst_gbox, resampling, dst_nodata, extra_dim_index)\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m progress_cbk:\n\u001b[1;32m     81\u001b[0m     progress_cbk(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/storage/_read.py:123\u001b[0m, in \u001b[0;36mread_time_slice\u001b[0;34m(rdr, dst, dst_gbox, resampling, dst_nodata, extra_dim_index)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" From opened reader object read into `dst`\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[39m:returns: affected destination region\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39massert\u001b[39;00m dst\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m dst_gbox\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 123\u001b[0m src_gbox \u001b[39m=\u001b[39m rdr_geobox(rdr)\n\u001b[1;32m    125\u001b[0m rr \u001b[39m=\u001b[39m compute_reproject_roi(src_gbox, dst_gbox)\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m roi_is_empty(rr\u001b[39m.\u001b[39mroi_dst):\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/datacube/storage/_read.py:32\u001b[0m, in \u001b[0;36mrdr_geobox\u001b[0;34m(rdr)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Construct GeoBox from opened dataset reader.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m h, w \u001b[39m=\u001b[39m rdr\u001b[39m.\u001b[39mshape\n\u001b[0;32m---> 32\u001b[0m \u001b[39mreturn\u001b[39;00m GeoBox(w, h, rdr\u001b[39m.\u001b[39mtransform, rdr\u001b[39m.\u001b[39;49mcrs)\n",
      "File \u001b[0;32m~/Code/mari-data/data/zarr/datacube-zarr/datacube_zarr/driver.py:75\u001b[0m, in \u001b[0;36mZarrDataSource.BandDataSource.crs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcrs\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m geometry\u001b[39m.\u001b[39mCRS:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mda\u001b[39m.\u001b[39;49mcrs\n",
      "File \u001b[0;32m~/Code/mari-data_harvest/.venv/lib/python3.10/site-packages/xarray/core/common.py:239\u001b[0m, in \u001b[0;36mAttrAccessMixin.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[39mwith\u001b[39;00m suppress(\u001b[39mKeyError\u001b[39;00m):\n\u001b[1;32m    238\u001b[0m             \u001b[39mreturn\u001b[39;00m source[name]\n\u001b[0;32m--> 239\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m    241\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataArray' object has no attribute 'crs'"
     ]
    }
   ],
   "source": [
    "ds_datacube = dc.load('waves', measurements=list(a['name'].values()), output_crs=\"epsg:4326\", resolution=(1, 1), align=(0.5, 0.5),crs='EPSG:4326')\n",
    "ds_datacube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "for var in vars:\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ds_datacube[var].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "nc = xarray.open_dataset('./data/waves1.nc', engine=\"netcdf4\")\n",
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "zarr = xarray.open_zarr('s3://52n-maridata/opendatacube/zarr/forecast/waves/')\n",
    "zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zarr.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.data_vars['spatial_ref']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
